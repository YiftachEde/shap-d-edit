{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aee5b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "None is the location\n",
      "None is the location\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b91017609d4568b75b2d994e0955af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents, sample_latents_noised\n",
    "# from shap_e.diffusion.gaussian_diffusion import ddim_inversion\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config,GaussianDiffusion\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "batch_size = 1\n",
    "guidance_scale = 12.5\n",
    "prompt = \"a high table\"\n",
    "\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")\n",
    "if hasattr(model, \"cached_model_kwargs\"):\n",
    "    model_kwargs = model.cached_model_kwargs(batch_size, dict(texts=[prompt] * batch_size))\n",
    "pass\n",
    "\n",
    "\n",
    "\n",
    "# latents_noised = diffusion.ddim_inversion(model=model,cond=model_kwargs['embeddings'],latent=latents,clip_denoised=True,model_kwargs=model_kwargs)\n",
    "# print(latents_noised.shape)\n",
    "# latents = ddim_inversion(latents,\n",
    "                        #  model, diffusion, progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8631189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:32<00:00, 31.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# diffusion = GaussianDiffusion(betas = torch.linspace(0.0009,0.0120,1000),model_mean_type=diffusion.model_mean_type,model_var_type=diffusion.model_var_type,loss_type=diffusion.loss_type,discretized_t0=diffusion.discretized_t0,channel_scales=diffusion.channel_scales,channel_biases=diffusion.channel_biases)\n",
    "latents_noised = diffusion.ddim_inversion(model=model,latent=latents,clip_denoised=True, model_kwargs=dict(texts=[prompt] * batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57405cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_new = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale*10,\n",
    "    model_kwargs=dict(texts=[\"a rainbow colored shark\"] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=False,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=20,\n",
    "    s_churn=0,\n",
    "    noise=latents_noised[300]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb5792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_new_2 = sample_latents_noised(\n",
    "    latent=latents_noised[100],\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[\"a re\"] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=False,\n",
    "    use_fp16=True,\n",
    "    use_karras=False,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f2d2daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.9395, -0.0048,  ..., -0.9854,  0.6816, -0.9404]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8473c6b0694e4c63b4eb00fb38577928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<img src=\"data:image/gif;base64,R0lGODlhAAEAAYcAAOvq6+vq6urq6+rq6urp6unq6unq6enp6unp6ejq6ejp6ujp6e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:41, 41.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9976, -0.9396,  0.0119,  ..., -0.9797,  0.6747, -0.9407]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:45, 46.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, latent \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(latents_noised[\u001b[38;5;241m0\u001b[39m:])):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(latent)\n\u001b[0;32m---> 11\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_latent_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     im \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m     13\u001b[0m     display(gif_widget(images))\n",
      "File \u001b[0;32m~/anaconda3/envs/instantmesh/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/util/notebooks.py:53\u001b[0m, in \u001b[0;36mdecode_latent_images\u001b[0;34m(xm, latent, cameras, rendering_mode)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_latent_images\u001b[39m(\n\u001b[1;32m     48\u001b[0m     xm: Union[Transmitter, VectorDecoder],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     rendering_mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[0;32m---> 53\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_views\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransmitter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_to_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAttrDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendering_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrendering_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     arr \u001b[38;5;241m=\u001b[39m decoded\u001b[38;5;241m.\u001b[39mchannels\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Image\u001b[38;5;241m.\u001b[39mfromarray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/nerstf/renderer.py:215\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_views\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    211\u001b[0m rendering_mode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_views_from_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rendering_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    225\u001b[0m     sdf_fn \u001b[38;5;241m=\u001b[39m tf_fn \u001b[38;5;241m=\u001b[39m nerstf_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/renderer.py:217\u001b[0m, in \u001b[0;36mrender_views_from_rays\u001b[0;34m(render_rays, batch, params, options, device)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[1;32m    213\u001b[0m     rays_batch \u001b[38;5;241m=\u001b[39m AttrDict(\n\u001b[1;32m    214\u001b[0m         rays\u001b[38;5;241m=\u001b[39mrays[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    215\u001b[0m         radii\u001b[38;5;241m=\u001b[39mradii[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size],\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(camera, DifferentiableProjectiveCamera):\n\u001b[1;32m    220\u001b[0m         z_batch \u001b[38;5;241m=\u001b[39m z_directions[:, idx \u001b[38;5;241m*\u001b[39m ray_batch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ray_batch_size]\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/nerstf/renderer.py:122\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_rays\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    113\u001b[0m options\u001b[38;5;241m.\u001b[39mnerf_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoarse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m parts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    115\u001b[0m     RayVolumeIntegral(\n\u001b[1;32m    116\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     ),\n\u001b[1;32m    121\u001b[0m ]\n\u001b[0;32m--> 122\u001b[0m coarse_results, samplers, coarse_raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_shared_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportance_sampling_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportance_sampling_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Then, render with additional importance-weighted ray samples.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m options\u001b[38;5;241m.\u001b[39mnerf_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/nerf/ray.py:79\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(rays, parts, void_model, shared, prev_raw_outputs, render_with_direction, importance_sampling_options)\u001b[0m\n\u001b[1;32m     74\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_i, prev_raw_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parts, prev_raw_outputs):\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Integrate over [t[i], t[i + 1]]\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     results_i \u001b[38;5;241m=\u001b[39m \u001b[43mpart_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_raw_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_with_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_with_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Create an importance sampler for (optional) fine rendering\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     samplers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     90\u001b[0m         ImportanceRaySampler(\n\u001b[1;32m     91\u001b[0m             results_i\u001b[38;5;241m.\u001b[39mvolume_range, results_i\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimportance_sampling_options\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/nerf/ray.py:238\u001b[0m, in \u001b[0;36mRayVolumeIntegral.render_rays\u001b[0;34m(self, origin, direction, t0, prev_raw, shared, render_with_direction)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# 1. Intersect the rays with the current volume and sample ts to\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# integrate along.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m vrange \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvolume\u001b[38;5;241m.\u001b[39mintersect(origin, direction, t0_lower\u001b[38;5;241m=\u001b[39mt0)\n\u001b[0;32m--> 238\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shared:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Append the previous ts now before fprop because previous\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# rendering used a different model and we can't reuse the output.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     ts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(torch\u001b[38;5;241m.\u001b[39mcat([ts, prev_raw\u001b[38;5;241m.\u001b[39mts], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/main/Research/shap-e/shap_e/models/nerf/ray.py:438\u001b[0m, in \u001b[0;36mStratifiedRaySampler.sample\u001b[0;34m(self, t0, t1, n_samples, epsilon)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m:param t0: start time has shape [batch_size, *shape, 1]\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m:param t1: finish time has shape [batch_size, *shape, 1]\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m:param n_samples: number of ts to sample\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m:return: sampled ts of shape [batch_size, *shape, n_samples, 1]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m ones \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(t0\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 438\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    441\u001b[0m     ts \u001b[38;5;241m=\u001b[39m t0 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ts) \u001b[38;5;241m+\u001b[39m t1 \u001b[38;5;241m*\u001b[39m ts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "                        # Example of saving the latents as meshes.\n",
    "from shap_e.util.notebooks import decode_latent_mesh\n",
    "render_mode = 'nerf' # you can change this to 'stf'\n",
    "size = 256 # this is the size of the renders; higher values take longer to render.\n",
    "from tqdm import tqdm\n",
    "cameras = create_pan_cameras(size, device)\n",
    "# latents_noised\n",
    "im = []\n",
    "for i, latent in tqdm(enumerate(latents_noised[0:])):\n",
    "    print(latent)\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    im += images\n",
    "    display(gif_widget(images))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d690c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diffusion.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "967e8816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6049b4ba11524850ab183cd7eaad1157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "# diffusion.betas = diffusion.betas[:200]\n",
    "diffusion.num_timesteps = 200\n",
    "diffusion.alphas_cumprod = diffusion.alphas_cumprod[:200]\n",
    "latents_new = sample_latents(\n",
    "    batch_size=2,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[\"a coffee table\"] * 2),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    "    noise=torch.cat((latents_noised[200],latents_noised[200]),dim=0)\n",
    ")\n",
    "\n",
    "                        # Example of saving the latents as meshes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e163a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1f8a5eeb994c11ad6af86d170ccd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<img src=\"data:image/gif;base64,R0lGODlhAAEAAYcAAFk9KVk9KFg9Klg9KVk8KFg8KVg8KFc8KVc8KFY8KVc7KFc7J1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:41, 41.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d887d45bf36c4d9a9c43572203247d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<img src=\"data:image/gif;base64,R0lGODlhAAEAAYcAAFk9KVk9KFg9Klg9KVk8KFg8KVg8KFc8KVc8KFY8KVc7KFc7J1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:22, 41.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from shap_e.util.notebooks import decode_latent_mesh\n",
    "render_mode = 'nerf' # you can change this to 'stf'\n",
    "size = 256 # this is the size of the renders; higher values take longer to render.\n",
    "from tqdm import tqdm\n",
    "cameras = create_pan_cameras(size, device)\n",
    "# latents_noised\n",
    "im = []\n",
    "for i, latent in tqdm(enumerate(latents_new[:2])):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    im += images\n",
    "    display(gif_widget(images))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Example of saving the latents as meshes.\n",
    "from shap_e.util.notebooks import decode_latent_mesh\n",
    "render_mode = 'nerf' # you can change this to 'stf'\n",
    "size = 128 # this is the size of the renders; higher values take longer to render.\n",
    "from tqdm import tqdm\n",
    "cameras = create_pan_cameras(size, device)\n",
    "# latents_noised\n",
    "im = []\n",
    "for i, latent in tqdm(enumerate(latents_noised[1022:])):\n",
    "    print(latent)\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    im += images\n",
    "    display(gif_widget(images))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7eac24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2453c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fvcore in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (1.26.3)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (0.1.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (2.1.0)\n",
      "Requirement already satisfied: Pillow in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (9.3.0)\n",
      "Requirement already satisfied: tabulate in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from iopath) (2.3.0)\n",
      "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html\n",
      "Collecting pytorch3d\n",
      "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/pytorch3d-0.7.5-cp310-cp310-linux_x86_64.whl (20.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fvcore in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from pytorch3d) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from pytorch3d) (0.1.9)\n",
      "Requirement already satisfied: numpy in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (1.26.3)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.1.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (2.1.0)\n",
      "Requirement already satisfied: Pillow in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (9.3.0)\n",
      "Requirement already satisfied: tabulate in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /home/yiftach/anaconda3/envs/instantmesh/lib/python3.10/site-packages (from iopath->pytorch3d) (2.3.0)\n",
      "Installing collected packages: pytorch3d\n",
      "Successfully installed pytorch3d-0.7.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "version_str=\"\".join([\n",
    "    f\"py3{sys.version_info.minor}_cu\",\n",
    "    torch.version.cuda.replace(\".\",\"\"),\n",
    "    f\"_pyt{pyt_version_str}\"\n",
    "])\n",
    "!pip install fvcore iopath\n",
    "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128 # this is the size of the renders; higher values take longer to render.\n",
    "cameras = create_pan_cameras(size, device)\n",
    "# latents_noised\n",
    "im2 = []\n",
    "for i, l2 in tqdm(enumerate(latents_new)):\n",
    "    images = decode_latent_images(xm, l2, cameras, rendering_mode=\"nerf\")\n",
    "    display(gif_widget(images))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d94e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "im[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c707e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = load_model('transmitter', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_new,latents,latents_noised[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = diffusion_from_config(load_config('diffusion'))\n",
    "batch_size = 1\n",
    "guidance_scale = 15.0\n",
    "prompt = \"a panda\"\n",
    "\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
